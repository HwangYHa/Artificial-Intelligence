{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HwangYHa/Artificial-Intelligence/blob/main/Data_collection_and_processing_for_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eahu9L47aq-M"
      },
      "source": [
        "# 데이터 수집 및 준비\n",
        "## 1. 인터넷에서 데이터 수집\n",
        "### 1.1 웹 스크래핑(Web scraping)\n",
        "인터넷은 자연어처리(Natural Language Processing, NLP) 프로젝트에 사용할 수 있는 풍부한 데이터를 제공합니다. 그러나 일반적인 데이터는 NLP 프로젝트에 바로 사용할 수가 없습니다. 쉽게 다운로드하고 추가로 작업을 쉽게할 수 있는 워드문서 또는 스프레드시트로 만들어지지 않았기 때문입니다. 따라서 자체적으로 데이터를 수집하고 처리하는 방법을 학습해야 합니다.\n",
        "이 노트북 연습에서는 '웹 스크래핑' 이라는 프로세스를 사용하여 웹 사이트에서 원시 데이터를 수집하는 방법을 학습합니다. 여러분이 원하는 인터넷 웹 사이트의 정보를 수집할 수 있는 웹스크래퍼를 구축할 것입니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXHbIA3gaq-P"
      },
      "source": [
        "### 1.1.1 필수 라이브러리/패키지 가져오기\n",
        "먼저, 우리는 필요한 패키지를 가져옵니다:\n",
        "- `requests` 패키지를 사용하면 파이썬 스크립트가 웹 사이트와 통신하고 해당 사이트에서 정보를 '요청'할 수 있습니다. \n",
        "- 뷰티풀 수프(beautiful soup)패키지는 `bs4`라고도 불리며 웹사이트에서 정보를 얻고 추출하는데 유용한 기능을 제공합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXrkcqjXaq-Q"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import bs4\n",
        "\n",
        "print (\"requests version 성공적으로 가져왔습니다 \"+requests.__version__)\n",
        "print (\"beautifulsoup version 성공적으로 가져왔습니다 \"+bs4.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDZDxKyHaq-R"
      },
      "source": [
        "네! 패키지를 가져왔습니다. 이제 몇몇 웹사이트를 스크래핑할 준비가 되었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TQTDg72aq-R"
      },
      "source": [
        "### 1.1.2 웹 사이트에 대한 정보 찾기\n",
        "\n",
        "이제 https://en.wikipedia.org/wiki/Jupiter 를 방문하여 페이지 내용을 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpcha3Ttaq-S"
      },
      "source": [
        "### 실습: HTML 코드를 보고 다양한 태그를 식별할 수 있나요?\n",
        "  \n",
        "`requests.get()` 함수를 사용하여 웹 사이트를 호출합니다. `request.get()` 함수는 'GET' 요청을 웹 사이트 주로소 전송하고, 응답을 검색하려고 시도합니다. `request.get()` 함수는 'Response [200]'을 반환해야 합니다. 이는 사이트에서 정보를 성공적으로 수신했음을 의미합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiPUVquPaq-S"
      },
      "outputs": [],
      "source": [
        "base_url = 'https://en.wikipedia.org/wiki/Jupiter'\n",
        "r = requests.get(base_url)\n",
        "r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cwY325Taq-T"
      },
      "source": [
        "### 작업: base_url을 'https://en.wikipedia.org/wiki/Jupiter' 에서 'https://en.wikipedia.org/wiki/Jus21' 으로 변경해 보겠습니다\n",
        "\n",
        "이러한 응답 코드는 연결 문제를 해결하는데 도움이 되지만, 지금은 응답 코드 200은 GET 요청이 성공했다는 것을 알기만 하면 됩니다!\n",
        "\n",
        "<em> 링크가 깨지거나 제대로 연결되지 않는 경우 위의 코드에서 `<Response [404]>` 를 받을 수 있습니다. </em>\n",
        "\n",
        "### 이제 base_url을 다시 'https://en.wikipedia.org/wiki/Jupiter' 로 변경하여 나머지 노트북에서 계속 사용합니다.\n",
        "\n",
        "연결이 설정되었습니다! 우리는 접근 할 수 있습니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AEXslp7aq-U"
      },
      "source": [
        "### 1.1.3 데이터를 얻기 위한 beautifulsoup 사용\n",
        "\n",
        "이제 `bs4`를 사용하여 웹 사이트에 요청하여 출력을 읽을 수 있습니다. \n",
        "\n",
        "뷰티풀 수프는 HTML, XML 및 기타 마크 업 언어에서 데이터를 가져오는 파이썬 라이브러리입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmcCd84maq-U"
      },
      "outputs": [],
      "source": [
        "#`r.text`에는 이전에 GET 요청을 했을 때 반환된 원시 HTML이 포함되어 있습니다. \n",
        "#`'html5lib'`는 BeautifulSoup에 HTML 정보를 읽고 있다고 알려줍니다. \n",
        "soup = bs4.BeautifulSoup(r.text,'html5lib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26FAxuv0aq-U"
      },
      "source": [
        "기본적으로 우리는 GET 요청에서 초기 응답을 받아 BeautifulSoup에 응답을 읽고 이해하도록 요청하고, 이 모든 것을 변수 `soup`으로 저장합니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFRk3Hwuaq-V"
      },
      "source": [
        "### 1.1.4 beautifulsoup을 사용하여 특정 태그 검색\n",
        "\n",
        "여기서부터 흥미로워집니다. BeautifulSoup은 특정 태그를 검색하는 데 사용할 수 있습니다.\n",
        "\n",
        "beautifulsoup을 사용하여 위키피디아(Wikipedia) 웹 사이트의 모든 제목을 찾아봅시다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9w9qjMokaq-V"
      },
      "outputs": [],
      "source": [
        "headers = []\n",
        "for url in soup.findAll(\"h3\"):\n",
        "    headers.append(url.text)\n",
        "    print(url.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I77UCLOFaq-V"
      },
      "source": [
        "이 코드 조각에서 우리는 모든 `<h3>` 태그를 찾습니다. 그런 다음 발견한 모든 태그에 루프를 사용하여 텍스트를 추출합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHoABK3yaq-W"
      },
      "outputs": [],
      "source": [
        "i = len(headers) - 1\n",
        "counter = 0\n",
        "while counter <= i:\n",
        "    if headers[counter].startswith('\\n'):\n",
        "        headers.pop(counter)\n",
        "        counter -= 1\n",
        "    counter += 1\n",
        "    i = len(headers) -1\n",
        "print(headers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-FfgRxqaq-W"
      },
      "source": [
        "이제 각 헤더 뒤의 단락 태그에서 각 제목의 단락 텍스트를 추출합니다. 하나의 헤더에서 대해 어떠한 방법으로 수행되는지 확인하고, 원하는 모든 헤더에 대해 수행할 수 있습니다.\n",
        "\n",
        "텍스트는 `<p>` 태그로 둘러싸여 있습니다. 이 정보를 사용하여 텍스트를 추출할 것입니다.\n",
        "  \n",
        "이 페이지에서 정보는 `<p>` 태그 내에 있습니다. 우리는 모든 단락을 찾고 싶기 때문에 `<p>` 태그를 찾아`.get_text()`를 호출하여 실제 단어를 검색합니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vl9zPnrtaq-W"
      },
      "outputs": [],
      "source": [
        "r = requests.get(base_url)\n",
        "soup = bs4.BeautifulSoup(r.text,'html5lib')\n",
        "deet = soup.find('h3', text = headers[0]) # 클래스 'entry-content content' 의 div 태그 검색\n",
        "para = deet.find_next_sibling('p') # 이 태그 안에서 모든 p 태그 검색\n",
        "print(para.get_text())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkt7N8BDaq-X"
      },
      "source": [
        "위의 코드에서는 `find_next_sibling` 함수를 사용하여 \"구성(Composition)\"으로 레이블된 헤더 다음에 오늘 단락 태그를 가져옵니다. 위의 코드 조각에서 하나의 단락만을 어떻게 얻을 수 있는지 보여줍니다.\n",
        "\n",
        "해결 방안으로 다음 태그들을 반복적으로 살펴보겠습니다.\n",
        "\n",
        "- 이웃 태그가 `<h2>` 또는 `<h3>` 태그인 경우다음 헤더로 이동합니다.\n",
        "\n",
        "- 태그가 `<p>` 태그인 경우에만 텍스트를 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec31G-Fnaq-X"
      },
      "outputs": [],
      "source": [
        "r = requests.get(base_url)\n",
        "soup = bs4.BeautifulSoup(r.text,'html5lib')\n",
        "deet = soup.find('h3', text = headers[0]) # 클래스 'entry-content content' 의 div 태그 검색\n",
        "\n",
        "for para in deet.find_next_siblings(): # 이 태그 안에서 모든 p 태그 검색\n",
        "    if para.name == \"h2\" or para.name == \"h3\":\n",
        "        break\n",
        "    elif para.name == \"p\":\n",
        "        print(para.get_text())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4TAG2lraq-X",
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [],
      "source": [
        "print(all_para)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-W0E-87aq-Y"
      },
      "source": [
        "### 1.2 데이터 저장\n",
        "축하합니다! 여러분 자신만의 스크레이퍼를 구축하고 여러 웹 페이지에서 데이터를 수집하는데 사용했습니다. 이제 데이터에 접근할 때마다 웹 사이트를 스크랩할 필요가 없도록 데이터를 저장하는 방법이 필요합니다. 수집된 정보를 저장하면 웹 사이트가 삭제되었거나 변경되는 경우에도 데이터가 보존됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LLNKMLaaq-Y"
      },
      "source": [
        "### 작업 2: 테이터를 텍스트 파일로 저장하기 위한 코드를 작성합니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtIQ0YrCaq-Y"
      },
      "outputs": [],
      "source": [
        "with open('./wiki.txt', 'wb') as file_handler:\n",
        "        file_handler.write(all_para.encode('utf8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV1g8acqaq-Y"
      },
      "source": [
        "이제 디렉토리 폴더에 있는 파일을 확인하고 필요한 정보가 수집되었는지 확인하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02x4s04gaq-Y"
      },
      "source": [
        "### 트윗 데이터 다운로드 \n",
        "지금은 트위터에서 소셜 미디어의 반응을 확인하는 데이터 세트를 살펴볼 것입니다.\n",
        "\n",
        "이 데이터 세트는 트윗이 재난과 관련이 있는지 없는지에 대한 표시로 구성됩니다.\n",
        "\n",
        "이 데이터 세트를 AI 시스템을 훈련시키는 데 어떻게 사용할 수 있을까요? 이 데이터 세트가 도움이 될 수 있을까요?\n",
        "\n",
        "트윗을 분류할 수 있게 되면 실제 재난 시 노이즈 정보를 걸려내고 유용한 트위 정보를 실시간으로 얻을 수 있습니다. 이러한 트윗에는 피해야 할 장소, 도움이되는 방법, 도움을 요청하는 방법 등 유용한 정보가 포함될 수 있습니다.\n",
        "\n",
        "아래 코드는 `urllib` 라이브러리를 사용하여 'url'에 있는 .csv 파일을 다운로드하고 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJpr2jZ2aq-Y"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "\n",
        "url = 'https://sl2files.sustainablelivinglab.org/DatasetSocialMedia-Disaster-tweets-DFE.csv'\n",
        "\n",
        "csv = urllib.request.urlopen(url).read()\n",
        "with open('./[Dataset]socialmedia disaster tweets DFE.csv', 'wb') as fx:\n",
        "    fx.write(csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ702nJ5aq-Z"
      },
      "source": [
        "이제 데이터를 다운로드했습니다! 방금 만든 csv 파일을 확인해 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyrsFhSZaq-Z"
      },
      "source": [
        "## 2.1 텍스트 처리\n",
        "원시(raw) 데이터 형태의 트윗은 텍스트의 문자열일 뿐입니다. 사람은 텍스트의 문자열을 쉽게 읽고 이해할 수 있지만, 컴퓨터는 그렇게 하는 데 훨씬 더 어려움을 겪습니다. 따라서 이러한 문자열에 대하여 컴퓨터가 인식하고 작업할 수 있는 형태로 분할하는 전처리를 수행해야 합니다.\n",
        "\n",
        "텍스트를 전처리하면 텍스트를 분석하고 시각화할 수 있으며 컴퓨터가 텍스트를 분류하는 데 사용할 수있는 트렌드 및 기능을 찾을 수 있습니다.\n",
        "\n",
        "트윗 재난에 대한 데이터 세트를 사용하여 연습을 해보겠습니다.  \n",
        "\n",
        "먼저 필요한 패키지를 가져옵니다:\n",
        "- pandas 패키지는 데이터 프레임이라는 구로조 파일을 프로그램에 로드하는 데 도움이됩니다.\n",
        "- nltk 패키지는 텍스트를 가공하고 시각화할 수 있도록 해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaOUyOnt1-vh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "print (\"pandas 버전을 성공적으로 가져왔습니다 \"+pd.__version__)\n",
        "print (\"nltk 버전을 성공적으로 가져왔습니다 \"+nltk.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sU2nIGWGaq-Z"
      },
      "outputs": [],
      "source": [
        "# 자연 재해에 대한 트윗이 포함된 csv 파일을 pandas dataframe으로 읽습니다.\n",
        "df_raw = pd.read_csv('[Dataset] socialmedia disaster tweets DFE.csv',encoding='ISO-8859-1')\n",
        "\n",
        "print (\"CSV 파일을 성공적으로 로드했습니다)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0DDb5J21-vh"
      },
      "outputs": [],
      "source": [
        "df_raw.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkLN_-Daaq-a"
      },
      "source": [
        "트윗의 몇 개의 무작위 표본을 살펴보겠습니다. .sample() 사용하여 샘플을 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UV_jxRhaq-a"
      },
      "outputs": [],
      "source": [
        "list(df_raw['text'].sample())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJEn1Yaqaq-a"
      },
      "source": [
        "### 텍스트 데이터를 중심으로\n",
        "\n",
        "이제 텍스트에 집중하기 위해 나머지 필요하지 않은 열은 제외하고 텍스트만 포함하는 새로운 데이터 프레임을 만들어 보겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6TdOQJfaq-b"
      },
      "outputs": [],
      "source": [
        "df_text = df_raw['text'].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C8GbmGAaq-b"
      },
      "source": [
        "## 2.2 NLP 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7Amau20aq-b"
      },
      "source": [
        "### 데이터 정리\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOr8IrfNaq-b"
      },
      "source": [
        "욕설(profanity) 필터링은 데이터 전처리 과정에서 매우 중요한 단계입니다. AI 모델이 좋은 단어와 나쁜 단어를 구분할 수 없기 때문에 예측 데이터를 순진하게 욕설 등의 나쁜 단어로 출력할 수 있습니다. 따라서 정리된 좋은 단어의 데이터만으로 모델을 훈련시키는 것이 필수적입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqaQn0Lkaq-b"
      },
      "outputs": [],
      "source": [
        "## 불필요한 단어 제거\n",
        "## 데이터 집합에서 'stupid'라는 단어가 발생하는 횟수를 확인\n",
        "frequecy_of_word=0\n",
        "for i in range(len(df_raw)):\n",
        "    x=df_raw[\"text\"][i]\n",
        "    if \"stupid\" in x:\n",
        "        frequecy_of_word+=1\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "print(\"바보라는 단어의 빈도는 :\",frequecy_of_word)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLfLRxH0aq-b"
      },
      "outputs": [],
      "source": [
        "for i in range(len(df_raw)):\n",
        "    x=df_raw['text'][i]\n",
        "    x=x.replace(\"stupid\",\"\") # 'stupid' 단어를 공백으로 대체\n",
        "    df_raw['text'][i]=x\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "frequecy_of_word=0\n",
        "for i in range(len(df_raw)):\n",
        "    x=df_raw[\"text\"][i]\n",
        "    if \"stupid\" in x:\n",
        "        frequecy_of_word+=1\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "print(\"바보라는 단어의 빈도는 :\",frequecy_of_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrREaRaEaq-b"
      },
      "source": [
        "### 토큰화\n",
        "\n",
        "NLP 작업의 첫 번째 단계는 트윗을 토큰화하는 것입니다. 토큰화가 무엇을 하는지 기억하십니까? 예를 이용하여 단일 트윗을 사용하여 각 프로세스가 수행하는 작업을 설명합니다. 토큰화는 nltk.tokenize.word_token(데이터)를 호출하여 수행됩니다.\n",
        "토큰화 전후의 텍스트를 출력하여 함수가 수행한 기능을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gThvhiwYaq-c"
      },
      "outputs": [],
      "source": [
        "sample_tweet = df_text.iloc[100]\n",
        "print('토큰화 전 :', sample_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D8kU8iQaq-c"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m62SI1saq-c"
      },
      "outputs": [],
      "source": [
        "tokenized_tweet = nltk.tokenize.word_tokenize(sample_tweet)\n",
        "print('토큰화 후 :', tokenized_tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRV_7deCaq-c"
      },
      "source": [
        "토큰화 과정을 통해 데이터에 어떤 작업을 진행하였습니까? 문장이 주어졌을 때 우리는 개별 단어를 쉽게 알 수 있지만 컴퓨터는 알 수 없습니다. 문장을 컴퓨터가 이해할 수 있는 단어로 분해할 필요가 있습니다.\n",
        "\n",
        "문장을 분해하는 것의 또 다른 이점은 말뭉치(텍스트 모음)에서 고유한 단어의 수를 측정할 수 있다는 것입니다. 이것은 말뭉치의 어휘로 알려져 있습니다. \n",
        "\n",
        "이를 위해 [목록 이해(list comprehension)](https://www.pythonforbeginners.com/basics/list-comprehensions-in-python) 라는 기술을 사용합니다. 목표는 모든 트윗에 토큰을 단일 목록에 넣은 후 해당 토큰 목록에서 고유 값을 찾는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQc7x230aq-c"
      },
      "outputs": [],
      "source": [
        "tokenized_raw = [nltk.tokenize.word_tokenize(x) for x in list(df_text)] # 각 트윗이 토큰 목록이 되는 트윗 목록 만들기\n",
        "len(set([y for x in tokenized_raw for y in x])) # 목록을 단일 목록으로 지정한 후 고유 토큰 수를 계산합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Eco9s4naq-c"
      },
      "source": [
        "이제 트윗 데이터 세트에 34,878개의 고유한 토큰이 있다는 것을 알게 되었습니다. 많은 단어처럼 보입니다! 텍스트 정규화의 목표 중 하나는 텍스트의 의미를 훼손하지 않고 중복된 단어를 제거하여 어휘의 단어 수를 줄이는 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESvsIgZNaq-c"
      },
      "source": [
        "### 어간 추출(Stemming)과 표제어 추출(Lemming)\n",
        "\n",
        "이제 동일한 샘플 트윗에 어간 추출(스테밍) 및 표제어 추출(레밍)을 수행하는 작업을 살펴보겠습니다. `nltk.stem.PorterStemmer()`과 `nltk.stem.WordNetLemmatizer()`를 사용합니다. 출력을 보고 위의 토큰화된 텍스트와 비교하여 각 도구의 차이점을 확인하십시오."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sublgUOaq-d"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download()\n",
        "\n",
        "porter = nltk.stem.PorterStemmer()\n",
        "wnl = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "stemmed = [porter.stem(word) for word in tokenized_tweet]\n",
        "print(stemmed)\n",
        "lemmed = [wnl.lemmatize(word) for word in tokenized_tweet]\n",
        "print(lemmed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKthLaNGaq-d"
      },
      "source": [
        "어떤 작업을 수행하였는지 말할 수 있습니까? 미묘한 [차이점](https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/) 을 확인해 보세요. \n",
        "\n",
        "스티머(stemmer)는 의미는 같지만 형태가 다른 단어들이 결국 같은 토큰으로 끝나도록 단어를 짧게 만들어 줍니다. 예를 들어, 'accident' 토큰은 'accid'로 줄어들었습니다.  'accidentally', 'accidental', 'accidents'와 같은 다른 비슷한 단어들은 모두 'accid'로 줄어들 것입니다.\n",
        "\n",
        "레마티저(lemmatizer)는 약간 다릅니다. 동일한 의미의 단어를 검색하고 어근으로 대체합니다. 즉, 동일한 의미를 가진 단어들은 컴퓨터가 하나의 단어로 이해할 수 있도록 합니다. 레머(lemmer)의 단점은 실행하는데 오랜 시간이 걸릴 수 있다는 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej9swK4aaq-d"
      },
      "source": [
        "### 금지어(stop-word) 제거\n",
        "다음으로, 금지어를 제거하여 텍스트를 정규화합니다. 금지어가 무엇인지 기억하십니까? nltk에서 금지어 목록을 제공하는데, 무엇이 포함되고 있는지 확인해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oftvq3_laq-d"
      },
      "outputs": [],
      "source": [
        "stop = nltk.corpus.stopwords.words('english')\n",
        "# 여기에서 제거하고 싶은 금지어를 추가합니다.\n",
        "stop.append('@')\n",
        "stop.append('#')\n",
        "stop.append('http')\n",
        "stop.append(':')\n",
        "stop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfwM4IZdaq-d"
      },
      "source": [
        "이제 각 코드 조각들이 수행하는 작업을 알고 있으므로, 데이터 프레임을 가져와서 토큰화된 트윗을 반환하는 함수를 만들 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYcghOt6aq-d"
      },
      "outputs": [],
      "source": [
        "def process_tweet(tweet):\n",
        "    tokenized_tweet = nltk.tokenize.word_tokenize(tweet)\n",
        "    stemmed = [porter.stem(word) for word in tokenized_tweet]\n",
        "    processed = [w.lower() for w in stemmed if w not in stop]\n",
        "    return processed\n",
        "\n",
        "def tokenizer(df):\n",
        "    tweets = []\n",
        "    for _, tweet in df.iterrows():\n",
        "        tweets.append(process_tweet(tweet['text']))\n",
        "    return tweets\n",
        "\n",
        "tweets = tokenizer(df_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkc0_Yphaq-d"
      },
      "source": [
        "잘라낸 단어가 많다는 것을 알 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VArjklImaq-e"
      },
      "outputs": [],
      "source": [
        "tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DANOwcsEaq-e"
      },
      "source": [
        "이러한 전처리 단계를 통해 원본 트윗 데이터를 가져와 개별 토큰으로 잘라내고 금지어를 제거하고, 형태소를 분석하여 모두 소문자로 만들었습니다. 이렇게 처리하여 10,000 토큰에 가까게 단어 크기를 줄였습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PqY-0Gvaq-e"
      },
      "source": [
        "## 데이터 시각화\n",
        "\n",
        "데이터를 시각화해 봅시다. 각 트윗의 단어 수 분포를 살펴볼 것입니다. 이 작업은 히스토그램을 사용하여 수행할 수 있습니다. 트윗에서 가장 일반적인 단어의 수는 무엇입니까? 데이터 세트를 분류하는 과정에서 모델은 트윗의 길리, 단어 수 등과 같은 요소를 확인할 수 있습니다. 데이터를 플로팅하면 코드가 정상적으로 동작하는지 확인하고, 터무니없는 숫자를 결과로 내놓지 않도록 방지하는데 도움을 줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ivh1ELy7aq-e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_hist(tweets):\n",
        "    sentence_lengths = [len(tokens) for tokens in tweets]\n",
        "    fig = plt.figure(figsize=(6, 6)) \n",
        "    plt.xlabel('Tweet length')\n",
        "    plt.ylabel('Number of tweets')\n",
        "    plt.hist(sentence_lengths, bins=20)\n",
        "    plt.show()\n",
        "    return sentence_lengths\n",
        "tweet_lengths = plot_hist(tweets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aYuu94Gaq-e"
      },
      "source": [
        "데이터에 대해 점점 더 많이 알아가고 있습니다. 더 깊이 들어가 관련 트윗과 관련 없는 트윗의 평균 길이에 차이가 있는지 확인할 수 있는지 살펴보겠습니다. 그것은 향후 분석에서 좋은 특성으로 사용할 수 있을 것입니다.\n",
        "\n",
        "이를 위해 `df_raw`에서 'choose_one' 열을 사용합니다. 열의 값이 'Relevant'인 경우 트윗은 자연 재해와 관련된 것으로 분류됩니다. 값이 'Not Relevant'인 경우 트윗은 자연 재해와 관련이 없습니다.\n",
        "  \n",
        "관련 트윗의 평균 길이와 관련 없는 트윗의 평균 길이를 출력할 수 있습니까?\n",
        "  \n",
        "* 힌트 1: 데이터 프레임 `df_result`는 처리 된 트윗과 관련성을 포함하도록 구성되었습니다. 'Relevant' 트윗만 포함된 테이터 프레임과, 'Not Relevant' 트윗만 포함된 데이터 프레임으로 2개를 만듭니다. 이 작업은 데이터 프레임을 부분 집합화하여 수행할 수 있습니다: `df_result[df_result['choose_one']=='Relevant']` 관련성 있는 트윗의 데이터 프레임을 만드는 예제입니다. 관련성 없는 트윗의 데이터 프레임을 만들어 보세요.\n",
        "\n",
        "* 힌트 2: 각 트윗의 길이를 계산하려면 `.apply(len)` 함수를 사용할 수 있습니다. `.mean()`를 사용하여 결과의 평균을 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDbtshuZaq-f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df_result = df_raw['choose_one'].copy().to_frame()\n",
        "df_result['processed_text'] = tweets\n",
        "df_neg=df_result[df_result['choose_one']=='Not Relevant']\n",
        "df_pos=df_result[df_result['choose_one']=='Relevant']\n",
        "print(df_pos['processed_text'].apply(len).mean(), df_neg['processed_text'].apply(len).mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBQ2Ye5Raq-f"
      },
      "source": [
        "트윗 길이가 좋은 특성이라고 생각하십니까?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfomJrfUaq-f"
      },
      "source": [
        "다음 내용을 배웠습니다:\n",
        "1. 텍스트 데이터를 찾는 방법\n",
        "2. 스크랩과 다양한 모듈을 사용하여 온라인으로 데이터를 수집하는 방법\n",
        "3. 이 데이터를 컴퓨터에 저장하는 방법\n",
        "4. 이 텍스트 데이터를 컴퓨터가 이해할 수 있는 양식으로 처리하는 방법\n",
        "5. 처리된 데이터를 분석하고 시각화하는 방법"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}